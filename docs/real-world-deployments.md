# ğŸš€ Real-World Muon Deployments

> Where Muon is being used in production and what we can learn from each deployment

## Major Deployments

### ğŸŒ™ Kimi K2 (Moonshot AI)

**The largest known Muon deployment**

| Specification | Details |
|--------------|---------|
| **Organization** | Moonshot AI (China) |
| **Release Date** | July 2025 |
| **Total Parameters** | 1 Trillion |
| **Active Parameters** | 32 Billion (MoE) |
| **Training Tokens** | 15.5 Trillion |
| **Architecture** | Mixture of Experts |

#### How They Used Muon

```
Configuration (from paper):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Muon Parameters:                       â”‚
â”‚  - All expert MLP weights               â”‚
â”‚  - Attention Q, K, V, O projections     â”‚
â”‚  - Learning rate: Not disclosed         â”‚
â”‚                                         â”‚
â”‚  Adam Parameters:                       â”‚
â”‚  - Embeddings                           â”‚
â”‚  - Router weights                       â”‚
â”‚  - Normalization layers                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key Insights

1. **MoE compatibility:** Muon works well with Mixture of Experts, even with sparse activation patterns

2. **Scaling behavior:** Benefits persist at trillion-parameter scale

3. **Expert training:** Each expert's weights benefit from orthogonalized updates

4. **Production validation:** If Moonshot trusts it for their flagship model, it's production-ready

---

### âš¡ NanoGPT Speedrun Records

**The benchmark that made Muon famous**

| Record | Configuration | Time | Date |
|--------|--------------|------|------|
| GPT-2 124M to val=3.28 | Muon + optimized arch | 2.92 A100-hrs | 2024 |
| Previous (Adam) | Standard training | 3.94 A100-hrs | 2024 |
| **Speedup** | | **1.35x** | |

#### Speedrun Configuration

```python
# From the record-setting runs
optimizer = Muon(
    muon_params=model.transformer_blocks.parameters(),
    lr=0.02,
    momentum=0.95,
    adamw_params=model.embeddings.parameters(),
    adamw_lr=3e-4,
)

# Key modifications that work with Muon:
# - Rotary embeddings
# - RMSNorm (not LayerNorm)
# - No bias terms
# - Flash attention
```

#### Leaderboard Links

- ğŸ“Š [NanoGPT Speedrun Leaderboard](https://github.com/KellerJordan/modded-nanogpt#leaderboard)
- ğŸ’» [Modded NanoGPT Repo](https://github.com/KellerJordan/modded-nanogpt)

---

### ğŸ”¬ Research Labs & Startups

**Known Muon Adopters**

| Organization | Use Case | Scale | Status |
|-------------|----------|-------|--------|
| Moonshot AI | Kimi K2 production | 1T params | âœ… Production |
| OpenAI (rumored) | Internal experiments | Unknown | ğŸ”¬ Research |
| Academic labs | Reproducibility studies | 1B-7B | âœ… Published |
| Various startups | Cost reduction | 100M-1B | âœ… Production |

---

## Case Studies

### Case Study 1: 7B Model Training

**Scenario:** Research lab training a 7B LLM

```
Before (AdamW):
- Training time: 14 days on 8x H100
- Hyperparameter tuning: 3 runs to find good lr
- Final loss: 2.45

After (Muon + AdamW hybrid):
- Training time: 11 days on 8x H100 (21% faster)
- Hyperparameter tuning: 1 run (less sensitive)
- Final loss: 2.43 (slightly better)
```

**Lessons:**
1. Speed gains hold at 7B scale
2. Less hyperparameter sensitivity saves tuning runs
3. Slightly better final loss is common

### Case Study 2: Vision Transformer

**Scenario:** ViT-Large for image classification

```
Architecture:
- Patch size: 16x16
- Embedding dim: 1024
- Layers: 24
- Heads: 16

Muon configuration:
- Q, K, V, O projections: Muon (lr=0.01)
- MLP weights: Muon (lr=0.01)
- Patch embedding: Adam (lr=1e-4)
- Class token, pos embed: Adam (lr=1e-4)
- LayerNorm: Adam (lr=1e-4)

Results:
- ImageNet top-1: 86.2% â†’ 86.4%
- Training time: 2.3 days â†’ 1.9 days (17% faster)
```

### Case Study 3: Diffusion Model

**Scenario:** Text-to-image diffusion model

```
Findings:
- Muon works for UNet linear layers
- Cross-attention layers benefit significantly
- Time embeddings should use Adam
- Overall: ~15% training speedup
```

---

## Deployment Checklist

### Before Production

- [ ] **Validate on subset:** Train small model with Muon first
- [ ] **Compare baselines:** Ensure Muon matches/beats Adam
- [ ] **Stress test:** Run for full duration to check stability
- [ ] **Checkpoint compatibility:** Verify save/load works correctly
- [ ] **Distributed testing:** Test at target cluster scale

### In Production

- [ ] **Monitor gradient stats:** Track norms pre/post orthogonalization
- [ ] **Log optimizer state:** Momentum buffers for debugging
- [ ] **Checkpoints frequently:** More often during first 10% of training
- [ ] **Have rollback plan:** Keep Adam config ready just in case

---

## Common Production Configurations

### Configuration 1: Transformer LLM

```python
# Typical LLM setup
MUON_CONFIG = {
    "params": [
        "transformer.*.attn.{q,k,v,o}_proj.weight",
        "transformer.*.mlp.{up,down,gate}_proj.weight",
    ],
    "lr": 0.02,
    "momentum": 0.95,
}

ADAM_CONFIG = {
    "params": [
        "embed_tokens.weight",
        "transformer.*.norm.weight",
        "lm_head.weight",
    ],
    "lr": 3e-4,
    "betas": (0.9, 0.95),
}
```

### Configuration 2: Vision Model

```python
# ViT setup
MUON_CONFIG = {
    "params": [
        "blocks.*.attn.{qkv,proj}.weight",
        "blocks.*.mlp.{fc1,fc2}.weight",
    ],
    "lr": 0.01,  # Slightly lower for vision
    "momentum": 0.95,
}

ADAM_CONFIG = {
    "params": [
        "patch_embed.proj.weight",
        "cls_token", "pos_embed",
        "blocks.*.norm*.weight",
        "norm.weight", "head.weight",
    ],
    "lr": 1e-4,
}
```

### Configuration 3: MoE Model

```python
# Mixture of Experts setup
MUON_CONFIG = {
    "params": [
        "transformer.*.attn.*.weight",
        "transformer.*.experts.*.{up,down}.weight",
    ],
    "lr": 0.02,
    "momentum": 0.95,
}

ADAM_CONFIG = {
    "params": [
        "embed_tokens.weight",
        "transformer.*.router.weight",  # Router stays with Adam
        "transformer.*.norm.weight",
    ],
    "lr": 3e-4,
}
```

---

## Performance Benchmarks

### Wall-Clock Speedups

| Model | Params | Hardware | Speedup vs Adam |
|-------|--------|----------|-----------------|
| GPT-2 Small | 124M | 1x A100 | 1.35x |
| GPT-2 Medium | 355M | 1x A100 | 1.32x |
| GPT-2 Large | 774M | 4x A100 | 1.30x |
| GPT-2 XL | 1.5B | 8x H100 | 1.33x |
| LLaMA-style 7B | 7B | 8x H100 | 1.25x |
| Kimi K2 | 1T | Unknown | Significant* |

*Exact speedup not disclosed

### Compute Overhead

```
Newton-Schulz overhead per step:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Size    â”‚ Overhead vs Adam       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 124M          â”‚ ~1.0%                  â”‚
â”‚ 1B            â”‚ ~0.8%                  â”‚
â”‚ 7B            â”‚ ~0.6%                  â”‚
â”‚ 70B           â”‚ ~0.4%                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Overhead DECREASES with scale because:
- Newton-Schulz is O(nÂ³) but sparse
- Forward/backward dominate at scale
- Muon overhead is negligible vs matmuls
```

---

## Lessons from the Field

### What Works

âœ… **Large transformer layers:** The bigger the better  
âœ… **Attention projections:** Consistent wins  
âœ… **MLP/FFN weights:** Strong benefits  
âœ… **Standard training loops:** Drop-in replacement  

### What Requires Care

âš ï¸ **Very small models (<10M):** Overhead may hurt  
âš ï¸ **Embeddings:** Always use Adam  
âš ï¸ **Custom architectures:** Test first  
âš ï¸ **Extreme learning rates:** Stay in 0.01-0.05 range  

### What Doesn't Work

âŒ **1D parameters:** Use Adam  
âŒ **Lookup tables/embeddings:** Use Adam  
âŒ **Very sparse gradients:** May not benefit  

---

ğŸ“š **References:**
- [Kimi K2 Technical Report](https://arxiv.org/abs/2507.15816)
- [NanoGPT Speedrun Repo](https://github.com/KellerJordan/modded-nanogpt)
- [Muon Reference Implementation](https://github.com/KellerJordan/Muon)
