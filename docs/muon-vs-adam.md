# âš”ï¸ Muon vs Adam: Complete Comparison

> A detailed head-to-head comparison of the two optimizers

## Quick Comparison Table

| Aspect | Muon | Adam/AdamW |
|--------|------|------------|
| **Philosophy** | Matrix-aware optimization | Element-wise optimization |
| **Gradient View** | Sees entire matrix structure | Treats each param independently |
| **Update Direction** | Orthogonalized (equal directions) | Scaled by gradient history |
| **Theoretical Basis** | Steepest descent under spectral norm | Adaptive learning rates |
| **Best For** | Linear layers, attention, MLPs | Embeddings, LayerNorm, 1D params |
| **Compute Overhead** | ~1% extra FLOPs | Baseline |
| **Memory** | Similar (stores momentum) | Stores momentum + variance |
| **Hyperparameters** | lr=0.02, momentum=0.95 | lr=3e-4, Î²â‚=0.9, Î²â‚‚=0.999, Îµ=1e-8 |

---

## The Core Difference

### Adam's Approach: Element-wise

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADAM UPDATE                               â”‚
â”‚                                                              â”‚
â”‚  Weight Matrix W:    Gradient G:       Update Î”W:           â”‚
â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”      â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”     â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”        â”‚
â”‚  â”‚wâ‚â‚â”‚wâ‚â‚‚â”‚wâ‚â‚ƒâ”‚      â”‚gâ‚â‚â”‚gâ‚â‚‚â”‚gâ‚â‚ƒâ”‚     â”‚Î”â‚â‚â”‚Î”â‚â‚‚â”‚Î”â‚â‚ƒâ”‚        â”‚
â”‚  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â†’   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â†’  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤        â”‚
â”‚  â”‚wâ‚‚â‚â”‚wâ‚‚â‚‚â”‚wâ‚‚â‚ƒâ”‚      â”‚gâ‚‚â‚â”‚gâ‚‚â‚‚â”‚gâ‚‚â‚ƒâ”‚     â”‚Î”â‚‚â‚â”‚Î”â‚‚â‚‚â”‚Î”â‚‚â‚ƒâ”‚        â”‚
â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜      â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜     â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜        â”‚
â”‚                                                              â”‚
â”‚  Each Î”áµ¢â±¼ computed INDEPENDENTLY based on gáµ¢â±¼ history       â”‚
â”‚                                                              â”‚
â”‚  Î”áµ¢â±¼ = -lr Ã— mÌ‚áµ¢â±¼ / (âˆšvÌ‚áµ¢â±¼ + Îµ)                              â”‚
â”‚                                                              â”‚
â”‚  âŒ No awareness that these form a LINEAR TRANSFORMATION    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Muon's Approach: Matrix-aware

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MUON UPDATE                               â”‚
â”‚                                                              â”‚
â”‚  Gradient G:           SVD Decomposition:                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”                  â”‚
â”‚  â”‚           â”‚    =    â”‚   â”‚ â”‚Ïƒâ‚   â”‚ â”‚   â”‚                  â”‚
â”‚  â”‚     G     â”‚         â”‚ U â”‚ â”‚  Ïƒâ‚‚ â”‚ â”‚V^Tâ”‚                  â”‚
â”‚  â”‚           â”‚         â”‚   â”‚ â”‚   Ïƒâ‚ƒâ”‚ â”‚   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”˜                  â”‚
â”‚                              â†“                               â”‚
â”‚                        Replace with 1s                       â”‚
â”‚                              â†“                               â”‚
â”‚  Update Î”W:            â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚ â”‚1    â”‚ â”‚   â”‚                  â”‚
â”‚  â”‚           â”‚    =    â”‚ U â”‚ â”‚  1  â”‚ â”‚V^Tâ”‚  = U Ã— V^T       â”‚
â”‚  â”‚   U V^T   â”‚         â”‚   â”‚ â”‚   1 â”‚ â”‚   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”˜                  â”‚
â”‚                                                              â”‚
â”‚  âœ… Preserves the DIRECTIONAL structure of the gradient     â”‚
â”‚  âœ… Gives EQUAL weight to all singular directions           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Why This Matters

### The "Rare Direction" Problem

```
Scenario: Your gradient is dominated by one direction

Adam sees:                         Muon sees:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     â”‚           â”‚                     â”‚
â”‚  Large gradient     â”‚           â”‚   Large gradient    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚           â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
â”‚                     â”‚           â”‚   Small gradient    â”‚
â”‚  Tiny gradient Â·    â”‚           â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
â”‚                     â”‚           â”‚   (amplified!)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Adam's update:                    Muon's update:
   Moves mostly in                   Moves EQUALLY in
   large gradient direction          both directions
```

**Result:** Muon learns "rare but important" features that Adam might miss or learn slowly.

---

## Performance Comparison

### Training Speed

| Benchmark | Muon Time | Adam Time | Speedup |
|-----------|-----------|-----------|---------|
| NanoGPT 124M (val=3.28) | 2.92 A100-hrs | 3.94 A100-hrs | **1.35x** |
| CIFAR-10 94% accuracy | 2.6 A100-sec | 3.3 A100-sec | **1.27x** |
| GPT-2 XL HellaSwag | 10 8xH100-hrs | 13.3 8xH100-hrs | **1.33x** |

### Hyperparameter Sensitivity

```
Adam:                               Muon:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ lr: 1e-4 to 1e-3       â”‚         â”‚ lr: 0.01 to 0.05       â”‚
â”‚ (sensitive!)           â”‚         â”‚ (more robust!)         â”‚
â”‚                        â”‚         â”‚                        â”‚
â”‚ Î²â‚: 0.9 (usually fixed)â”‚         â”‚ momentum: 0.95         â”‚
â”‚ Î²â‚‚: 0.95-0.999         â”‚         â”‚ (usually fixed)        â”‚
â”‚ Îµ: 1e-8 (important!)   â”‚         â”‚                        â”‚
â”‚                        â”‚         â”‚ NS iters: 5 (fixed)    â”‚
â”‚ 4 hyperparams to tune  â”‚         â”‚ 2 hyperparams to tune  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## When to Use Each

### Use Muon For âœ…

| Parameter Type | Example | Why |
|---------------|---------|-----|
| Linear layers | `nn.Linear(in, out)` | These ARE matrix operations |
| Attention QKV | `W_Q, W_K, W_V` | Matrix projections |
| MLP weights | FFN hidden layers | Dense transformations |
| Conv2d (reshaped) | Convolutional kernels | Can treat as 2D |

### Use Adam For âœ…

| Parameter Type | Example | Why |
|---------------|---------|-----|
| Embeddings | Token/position embeds | Not matrix operations |
| LayerNorm | Scale/shift params | 1D vectors |
| Biases | All bias terms | 1D vectors |
| Small params | < 256 elements | Overhead not worth it |

---

## Memory Usage

```
                Adam                           Muon
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             â”‚   â”‚                             â”‚
â”‚  For each parameter Î¸:      â”‚   â”‚  For each parameter Î¸:      â”‚
â”‚                             â”‚   â”‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”  â† Momentum (m)    â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”  â† Momentum        â”‚
â”‚  â”‚ Î¸   â”‚                    â”‚   â”‚  â”‚ Î¸   â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚  â””â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”  â† Variance (v)    â”‚   â”‚                             â”‚
â”‚  â”‚     â”‚                    â”‚   â”‚  (no variance needed!)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜                    â”‚   â”‚                             â”‚
â”‚                             â”‚   â”‚                             â”‚
â”‚  Memory: 3x param size      â”‚   â”‚  Memory: 2x param size      â”‚
â”‚                             â”‚   â”‚                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Hybrid Approach (Recommended)

```python
from muon import Muon

# Split parameters by type
muon_params = []
adam_params = []

for name, param in model.named_parameters():
    if param.ndim == 2 and param.shape[0] >= 256:
        muon_params.append(param)  # Use Muon
    else:
        adam_params.append(param)  # Use Adam

optimizer = Muon(
    muon_params=muon_params,
    lr=0.02,
    momentum=0.95,
    adamw_params=adam_params,
    adamw_lr=3e-4,
)
```

---

## Historical Context

```
Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
     â”‚                    â”‚                    â”‚
     â”‚                    â”‚                    â”‚
   2014                 2018                 2024
   Adam                 AdamW               Muon
   
   "Adaptive           "Weight decay        "Matrix-aware
    moment              done right"          optimization"
    estimation"
```

| Era | Insight |
|-----|---------|
| Pre-Adam | SGD + momentum works, but learning rate tuning is painful |
| Adam (2014) | Adapt learning rates per-parameter using gradient moments |
| AdamW (2017) | Fix weight decay (decouple from gradient scaling) |
| Muon (2024) | For matrices, work in matrix space, not element space |

---

## The Bottom Line

> **Adam:** "Treat every number independently, adapt based on history"
> 
> **Muon:** "This is a *matrix* â€” use matrix structure to optimize better"

For modern transformers with large linear layers, Muon consistently trains **25-35% faster** while requiring **fewer hyperparameters** to tune.

---

ğŸ“š **Further Reading:**
- [Understanding Muon](../articles/02-laker-newhouse-understanding.md)
- [Implementation Checklist](implementation-checklist.md)
- [Common Mistakes](common-mistakes.md)
