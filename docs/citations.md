# Muon Citations & References

> BibTeX citations for Muon-related papers and resources.

---

## üìÑ Primary Papers

### Old Optimizer, New Norm: An Anthology (The Theory Paper)

```bibtex
@article{bernstein2024anthology,
  title={Old Optimizer, New Norm: An Anthology},
  author={Bernstein, Jeremy and Newhouse, Laker},
  journal={arXiv preprint arXiv:2409.20325},
  year={2024},
  url={https://arxiv.org/abs/2409.20325}
}
```

### Muon at Scale: Kimi K2

```bibtex
@article{kimik2,
  title={Kimi K2: Open Agentic Intelligence},
  author={{Moonshot AI}},
  journal={arXiv preprint arXiv:2507.20534},
  year={2025},
  url={https://arxiv.org/abs/2507.20534}
}
```

### Training Neural Networks at Any Scale

```bibtex
@article{pethick2025training,
  title={Training Neural Networks at Any Scale with Scaling Blueprints},
  author={Pethick, Thomas and others},
  journal={arXiv preprint arXiv:2511.11163},
  year={2025},
  url={https://arxiv.org/abs/2511.11163}
}
```

### Muon is Scalable for LLM Training

```bibtex
@article{liu2025muonscalable,
  title={Muon is Scalable for LLM Training},
  author={Liu, Jingyuan and Su, Jianlin and others},
  journal={arXiv preprint arXiv:2502.16982},
  year={2025},
  url={https://arxiv.org/abs/2502.16982}
}
```

### Practical Efficiency of Muon for Pretraining

```bibtex
@article{essentialai2025muon,
  title={Practical Efficiency of Muon for Pretraining},
  author={{Essential AI}},
  journal={arXiv preprint arXiv:2505.02222},
  year={2025},
  url={https://arxiv.org/abs/2505.02222}
}
```

---

## üî¨ Foundational Work

### Shampoo Optimizer

```bibtex
@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned Stochastic Tensor Optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}
```

### Adam Optimizer

```bibtex
@article{kingma2014adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
```

### Spectral Norm Regularization

```bibtex
@inproceedings{miyato2018spectral,
  title={Spectral Normalization for Generative Adversarial Networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
```

### muP (Maximal Update Parameterization)

```bibtex
@inproceedings{yang2022tensor,
  title={Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
```

---

## üíª Implementations & Repositories

### Official Muon Repository

```bibtex
@misc{jordan2024muon,
  author={Jordan, Keller},
  title={Muon: An optimizer for hidden layers in neural networks},
  year={2024},
  howpublished={\url{https://github.com/KellerJordan/Muon}},
  note={Accessed: 2026-02-14}
}
```

### NanoGPT Speedrun

```bibtex
@misc{jordan2024nanogpt,
  author={Jordan, Keller},
  title={modded-nanogpt: NanoGPT speedrun repository},
  year={2024},
  howpublished={\url{https://github.com/KellerJordan/modded-nanogpt}},
  note={Accessed: 2026-02-14}
}
```

---

## üìù Blog Posts & Technical Writing

### Keller Jordan's Muon Post

```bibtex
@misc{jordan2024muonblog,
  author={Jordan, Keller},
  title={Muon: An optimizer for hidden layers in neural networks},
  year={2024},
  howpublished={\url{https://kellerjordan.github.io/posts/muon/}},
  note={Blog post. Accessed: 2026-02-14}
}
```

### Understanding Muon (Enter the Matrix)

```bibtex
@misc{newhouse2024understanding,
  author={Newhouse, Laker},
  title={Understanding Muon - Part 1: Enter the Matrix},
  year={2024},
  howpublished={\url{https://lakernewhouse.com/muon}},
  note={Blog post. Accessed: 2026-02-14}
}
```

### Deriving Muon from First Principles

```bibtex
@misc{bernstein2024deriving,
  author={Bernstein, Jeremy},
  title={Deriving Muon from First Principles},
  year={2024},
  howpublished={\url{https://jeremybernste.in/muon}},
  note={Blog post. Accessed: 2026-02-14}
}
```

---

## üìä Example Citation in Your Paper

If you use Muon in your research, we recommend citing:

**For the theory:**
```
We use the Muon optimizer (Bernstein & Newhouse, 2024) for training 
hidden layer parameters, based on spectral steepest descent principles.
```

**For large-scale results:**
```
Following the success of Muon at trillion-parameter scale (Moonshot AI, 2025),
we employ Muon for optimizing linear layer weights.
```

**For practical implementation:**
```
We use the official Muon implementation (Jordan, 2024) with learning rate 0.02
and 5 Newton-Schulz iterations.
```

---

## üìö Full Reference List

For a comprehensive reading list organized by topic, see:
- [Papers to Read](papers-to-read.md)
- [Real-World Deployments](real-world-deployments.md)

---

*Added: February 2026*
